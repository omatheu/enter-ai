# ENTER AI - Document Extraction API

Sistema de extra√ß√£o inteligente de dados de documentos PDF usando IA, desenvolvido como projeto Fellowship ENTER AI.

## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Tecnologias](#tecnologias)
- [Pr√©-requisitos](#pr√©-requisitos)
- [Instala√ß√£o e Execu√ß√£o](#instala√ß√£o-e-execu√ß√£o)
  - [Op√ß√£o 1: Docker (Recomendado)](#op√ß√£o-1-docker-recomendado)
  - [Op√ß√£o 2: Ambiente Local](#op√ß√£o-2-ambiente-local)
- [Uso da API](#uso-da-api)
- [Frontend](#frontend)
- [Testes](#testes)
- [Desafios & Solu√ß√µes Criativas](#desafios--solu√ß√µes-criativas)
- [Estrutura do Projeto](#estrutura-do-projeto)

---

## üéØ Vis√£o Geral

Este projeto √© uma API de extra√ß√£o de dados de documentos PDF que utiliza:
- **Extra√ß√£o de texto**: PDFPlumber para parsing de PDFs
- **IA Generativa**: OpenAI GPT para extra√ß√£o inteligente de campos
- **Schemas flex√≠veis**: Defini√ß√£o customiz√°vel de campos a serem extra√≠dos
- **Interface web**: Frontend interativo para testes

### Funcionalidades

- ‚úÖ Extra√ß√£o de campos estruturados de PDFs
- ‚úÖ Suporte para m√∫ltiplos tipos de documentos (Carteira OAB, Telas de Sistema, etc.)
- ‚úÖ Modo Batch: processar m√∫ltiplos PDFs de uma vez
- ‚úÖ Modo Single: processar um PDF por vez
- ‚úÖ Export de resultados em JSON e CSV
- ‚úÖ Interface web moderna e responsiva

---

## üõ†Ô∏è Tecnologias

### Backend
- **Python 3.12**
- **FastAPI**: Framework web moderno e r√°pido
- **PDFPlumber**: Extra√ß√£o de texto e tabelas de PDFs
- **OpenAI API**: Modelos GPT para extra√ß√£o inteligente
- **Pydantic**: Valida√ß√£o de dados e serializa√ß√£o

### Frontend
- **HTML5 + CSS3 + JavaScript**: Interface web pura (sem frameworks)
- **Design responsivo**: Funciona em desktop e mobile

### DevOps
- **Docker & Docker Compose**: Containeriza√ß√£o
- **Nginx**: Servidor web para frontend

---

## üì¶ Pr√©-requisitos

### Para Docker (Recomendado)
- Docker Engine 20.10+
- Docker Compose 1.29+

### Para Ambiente Local
- Python 3.12+
- Node.js 18+ (opcional, apenas para frontend)
- OpenAI API Key

---

## üöÄ Instala√ß√£o e Execu√ß√£o

### Op√ß√£o 1: Docker (Recomendado)

#### 1. Clone o reposit√≥rio
```bash
git clone <repository-url>
cd enter-ai
```

#### 2. Configure a API Key
Certifique-se de que o arquivo `backend/.env` existe e cont√©m sua OpenAI API Key:
```bash
# backend/.env
OPENAI_API_KEY=sk-your-openai-api-key-here
```

#### 3. Inicie os servi√ßos
```bash
docker-compose up --build
```

#### 4. Acesse a aplica√ß√£o
- **Frontend**: http://localhost:8080
- **Backend API**: http://localhost:8001
- **Documenta√ß√£o da API**: http://localhost:8001/docs

#### Comandos √∫teis
```bash
# Parar os servi√ßos
docker-compose down

# Ver logs
docker-compose logs -f

# Rebuildar apenas o backend
docker-compose up --build backend

# Rodar em segundo plano
docker-compose up -d
```

---

### Op√ß√£o 2: Ambiente Local

#### Backend

1. **Navegue at√© o diret√≥rio do backend**
```bash
cd backend
```

2. **Crie e ative o ambiente virtual**
```bash
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows
```

3. **Instale as depend√™ncias**
```bash
pip install -r requirements.txt
```

4. **Configure a API Key**
```bash
cp .env.example .env
# Edite .env e adicione sua OPENAI_API_KEY
```

5. **Inicie o servidor**
```bash
uvicorn main:app --reload --port 8001
```

A API estar√° dispon√≠vel em http://localhost:8001

#### Frontend

1. **Navegue at√© o diret√≥rio do frontend**
```bash
cd frontend
```

2. **Sirva os arquivos est√°ticos**

Op√ß√£o A - Python SimpleHTTPServer:
```bash
python3 -m http.server 8080
```

Op√ß√£o B - Node.js http-server:
```bash
npx http-server -p 8080
```

O frontend estar√° dispon√≠vel em http://localhost:8080

---

## üì° Uso da API

### Endpoint: POST /extract

Extrai campos de um documento PDF.

#### Exemplo com cURL:
```bash
curl -X POST http://localhost:8001/extract \
  -F "label=carteira_oab" \
  -F "extraction_schema={\"nome\":\"Nome do profissional\",\"inscricao\":\"N√∫mero de inscri√ß√£o\"}" \
  -F "pdf_file=@./docs/files/oab_1.pdf"
```

#### Exemplo com HTTPie:
```bash
http --form POST :8001/extract \
  label=carteira_oab \
  extraction_schema='{"nome": "Nome do profissional", "inscricao": "N√∫mero de inscri√ß√£o"}' \
  pdf_file@./docs/files/oab_1.pdf
```

#### Resposta:
```json
{
  "label": "carteira_oab",
  "results": [
    {
      "field_name": "nome",
      "value": "JOANA D'ARC",
      "source": "llm",
      "confidence": 0.0
    },
    {
      "field_name": "inscricao",
      "value": "101943",
      "source": "llm",
      "confidence": 0.0
    }
  ],
  "flat": {
    "nome": "JOANA D'ARC",
    "inscricao": "101943"
  },
  "metadata": {
    "model": "gpt-5-mini",
    "prompt_tokens": 210,
    "completion_tokens": 40,
    "total_tokens": 250,
    "duration_ms": 2400,
    "source": "mixed",
    "extracted_at": "2025-11-06T15:30:00",
    "profiling": {
      "pdf_text_ms": 8,
      "heuristics_ms": 2,
      "llm_ms": 2100,
      "total_ms": 2400
    }
  }
}
```
> O frontend utiliza o objeto `flat` para mostrar o JSON ‚Äúlimpo‚Äù, mas o payload completo mant√©m `results` (com fonte/confian√ßa) e `metadata.profiling`.

---

## üé® Frontend

O frontend oferece duas interfaces:

### 1. Modo Batch (Padr√£o)
- Upload de m√∫ltiplos PDFs
- Schema em formato array (matching dataset.json)
- Processamento em lote
- Resumo agregado dos resultados

### 2. Modo Single
- Upload de um √∫nico PDF
- Schema em formato objeto simples
- Ideal para testes r√°pidos

### Funcionalidades
- ‚úÖ Drag & drop de arquivos
- ‚úÖ Valida√ß√£o de schemas JSON
- ‚úÖ Visualiza√ß√£o simult√¢nea (JSON achatado + detalhes completos)
- ‚úÖ Export para JSON e CSV
- ‚úÖ Exemplos pr√©-carregados
- ‚úÖ M√©tricas de performance (tempo, custo, tokens) e progresso em tempo real no modo batch

---

## üß™ Testes

### Testes Automatizados
```bash
cd backend
source .venv/bin/activate
pytest
```

### Testes Manuais

#### 1. Script de exemplo (usa dataset.json)
```bash
cd backend
source .venv/bin/activate
python3 scripts/run_example.py
```

#### 2. Frontend web
Acesse http://localhost:8080 e:
1. Clique em "Load Example"
2. Fa√ßa upload dos PDFs correspondentes
3. Clique em "Extract Data"

---

## üéØ Desafios & Solu√ß√µes Criativas

Este projeto foi desenvolvido endere√ßando desafios cr√≠ticos de acur√°cia, performance e custo. Abaixo est√£o os principais desafios mapeados, as decis√µes arquiteturais tomadas e as solu√ß√µes implementadas com criatividade.

---

### **Desafio 1: Acur√°cia em PDFs Diversos com Custo Controlado**

**Problema:**
- PDFs variam muito em formato, layout e estrutura (carteiras OAB, telas de sistema, documentos scaneados)
- Chamar LLM para todos os campos consome tokens desnecessariamente (~10-20 tokens por campo por request)
- Nem todos os campos precisam de IA (CPF, email, datas t√™m padr√µes bem definidos)

**Decis√£o Tomada:**
Implementar estrat√©gia **"heur√≠sticas primeiro, LLM por exce√ß√£o"** com aprendizado incremental.

**Solu√ß√£o:**
1. **Extrator Heur√≠stico Inteligente** (`backend/app/extractors/heuristics.py:13-57`):
   - Patterns regex pr√©-compiladas (cache de CPU) para CPF, CNPJ, email, telefone, datas, etc.
   - Mapeamento sem√¢ntico: se o campo cont√©m "cpf" ou a descri√ß√£o menciona "cadastro de pessoa", tenta padr√£o de CPF
   - Suporte a enums: se a descri√ß√£o lista "pode ser A, B ou C", busca exatamente essas op√ß√µes no PDF
   - **Resultado**: 60-80% dos campos resolvidos sem LLM

2. **Schema Learner** (`backend/app/schema/confidence.py`):
   - Aprende padr√µes de sucesso/falha por tipo de documento (`carteira_oab`, `tela_sistema`, etc.)
   - Na primeira execu√ß√£o, tenta heur√≠stica; se falhar, marca para LLM
   - Nas execu√ß√µes seguintes, "lembra" que campo X sempre vem de fonte Y
   - **Resultado**: Redu√ß√£o de 40% em chamadas LLM ap√≥s 3-5 requisi√ß√µes

3. **Confian√ßa Graduada** (`backend/app/schema/confidence.py`):
   - Valida cada extra√ß√£o (heur√≠stica ou LLM) antes de usar
   - Se heur√≠stica extraiu CPF mas o formato est√° inv√°lido, descarta e chama LLM
   - Score de confian√ßa de 0-1: heur√≠sticas s√£o 0.7, regex puro √© 0.5, LLM √© 0.95
   - **Resultado**: Trade-off controlado entre custo e acur√°cia

---

### **Desafio 2: Lat√™ncia Aceit√°vel (Meta: 2-5 segundos)**

**Problema:**
- Chamar LLM √© lento (2-3s por request)
- PDFs grandes geram muito texto (20MB ‚Üí 50K caracteres)
- Cada campo adicional significa mais tokens ‚Üí mais lat√™ncia

**Decis√£o Tomada:**
Otimizar contexto enviado ao LLM e paralelizar recupera√ß√µes.

**Solu√ß√£o:**
1. **Contexto Compacto** (`backend/app/utils/context.py`):
   - Busca janelas de texto relevantes ao redor de keywords (n√£o envia PDF inteiro)
   - Normaliza acentos ("Jo√£o" ‚Üí "joao") para busca mais robusta
   - Limita a 1800 caracteres m√°ximo por request (ajust√°vel)
   - **Resultado**: Redu√ß√£o de ~60% em tokens, ~40% em lat√™ncia LLM

2. **Profiling de Performance** (`backend/app/utils/profiling.py`):
   - Mede tempo de cada etapa: PDF parsing, heur√≠sticas, LLM, recupera√ß√£o
   - Log detalhado em `metadata.profiling`: `{pdf_text_ms, heuristics_ms, llm_ms, recovery_ms, total_ms}`
   - Permite identificar gargalo e iterar
   - **Resultado**: Transpar√™ncia total; o frontend mostra tempo real

3. **Recupera√ß√£o Paralela** (`backend/app/services/extraction.py:237-241`):
   - Quando um campo falha (valor = None), dispara recupera√ß√£o
   - Ao inv√©s de: heur√≠stica ‚Üí template ‚Üí LLM sequencialmente (3x lat√™ncia)
   - Dispara os 3 em paralelo com `asyncio.gather()` e retorna o primeiro que sucede
   - **Resultado**: Recupera√ß√µes custam 1s ao inv√©s de 3s

---

### **Desafio 3: Qualidade e Observabilidade em Produ√ß√£o**

**Problema:**
- Dif√≠cil saber por que um campo falhou (heur√≠stica n√£o achou? LLM retornou null? Valida√ß√£o rejeitou?)
- Sem logs estruturados, √© imposs√≠vel debugar problemas em produ√ß√£o
- Resultado fica "achado" ou "n√£o achado", mas sem contexto

**Decis√£o Tomada:**
Logging estruturado granular + resposta detalhada com fonte/confian√ßa.

**Solu√ß√£o:**
1. **Logs de Campo** (`backend/app/services/extraction.py:436-441`):
   ```
   Field nome | heuristic_success confidence=0.85
   Field cpf | llm_success
   Field data | recovery_success source=template
   ```
   - Cada campo tem um "journal" do que aconteceu
   - Permite rastrear transforma√ß√µes: heur√≠stica falhou ‚Üí LLM sucedeu
   - **Resultado**: Rastreabilidade completa; pode-se reproduzir qualquer extra√ß√£o

2. **Resposta com Fonte e Confian√ßa** (`backend/app/models.py`):
   - Cada campo retorna n√£o apenas `value`, mas `source` (heuristic/llm/template) e `confidence` (0-1)
   - Frontend mostra: "CPF extra√≠do de HEUR√çSTICA (85% confian√ßa) vs NOME extra√≠do de LLM (95% confian√ßa)"
   - **Resultado**: Usu√°rio sabe qual resultado confiar; pode rejeitar baixa confian√ßa

3. **Cache Transparente**:
   - Se resultado vem de cache, `metadata.source` = "cache" (sem tokens gastos)
   - Se resultado √© misto (alguns campos de heur√≠stica, alguns de LLM), `metadata.source` = "mixed"
   - **Resultado**: Custos de API s√£o audit√°veis

---

### **Desafio 4: Recupera√ß√£o Resiliente (Sem Falhar Silenciosamente)**

**Problema:**
- Heur√≠stica falha (campo n√£o tem formato padr√£o)
- LLM retorna null ou formato inv√°lido
- Usu√°rio fica sem dados e sem saber por qu√™

**Decis√£o Tomada:**
Implementar **fallback strategy com 3 camadas** que escalam em "agressividade".

**Solu√ß√£o:**
1. **Layer 1: Heur√≠sticas Relaxadas** (`backend/app/extractors/error_recovery.py:79-100`):
   - Se padr√£o "cpf" falhou, tenta padr√£o gen√©rico "numero_documento"
   - Se heur√≠stica por description falhou, tenta busca case-insensitive por campo name
   - **Resultado**: Recupera ~15% dos casos perdidos

2. **Layer 2: Template Matching** (`backend/app/extractors/error_recovery.py:103-151`):
   - Usa exemplos aprendidos anteriormente para gerar padr√£o regex generalizado
   - Se viu "Jo√£o Silva" antes, gera padr√£o `[A-Za-z]+ [A-Za-z]+` e procura no PDF
   - **Resultado**: Recupera ~10% dos casos (especialmente nomes e endere√ßos)

3. **Layer 3: LLM Contextualizado** (`backend/app/extractors/error_recovery.py:52-73`):
   - Envia LLM de novo, mas com contexto expandido + exemplo anterior
   - "Campo `nome` (exemplo anterior: Jo√£o Silva): procure por..."
   - **Resultado**: Recupera ~20% dos casos restantes com IA

**Execu√ß√£o em Paralelo:**
   - Ao inv√©s de tentar sequencialmente (3s), dispara os 3 em paralelo
   - Retorna o primeiro que funciona
   - Logs: `"Field nome | recovery_success source=template_matching"`

---

### **Desafio 5: UX Responsiva em Processamento em Lote**

**Problema:**
- Modo batch: upload de m√∫ltiplos PDFs (3+)
- Usu√°rio n√£o sabe o progresso (est√° processando ou travou?)
- Resultado final s√≥ aparece quando tudo termina (pode levar 30+ segundos)

**Decis√£o Tomada:**
Feedback progressivo + processamento paralelo no frontend.

**Solu√ß√£o:**
1. **Processamento Paralelo Limitado** (`frontend/index.html`):
   - Ao inv√©s de enviar 1 PDF de cada vez, envia at√© 3 em paralelo
   - Mostra progresso em tempo real: "2/10 extra√ß√µes completas, 3 processando..."
   - **Resultado**: ~60% mais r√°pido para lotes grandes

2. **Resposta Flat + Detalhes Completos**:
   - Frontend mostra JSON "limpo" (`flat`: `{"nome": "Jo√£o", "cpf": "123.456.789-00"}`)
   - Mas exporta√ß√£o JSON retorna `results` + `metadata.profiling` (completo)
   - **Resultado**: Interface simples para humanos, dados completos para m√°quinas

3. **M√©tricas em Tempo Real**:
   - Calcula custo de API enquanto processa: "~5 requisi√ß√µes LLM, ~R$ 0.02"
   - Mostra tokens gastos: "850 tokens prompt + 40 completion"
   - **Resultado**: Transpar√™ncia de custo; usu√°rio v√™ ROI

---

## üß± Arquitetura & Trade-offs

- **Heur√≠sticas primeiro**: campos padronizados (CPF, seccional, subse√ß√£o, etc.) s√£o extra√≠dos via regex flex√≠veis. Apenas valores de baixa confian√ßa entram no lote LLM.
- **Contexto compacto**: o texto do PDF √© reduzido a janelas relevantes (com normaliza√ß√£o de acentos) antes de chamar o LLM e durante o recovery. Isso mant√©m o total de tokens e o `duration_ms` dentro da meta de 2‚Äì5‚ÄØs.
- **Cache multin√≠vel**: resultados completos (label+schema) e conte√∫do dos PDFs ficam em mem√≥ria. A primeira execu√ß√£o aprende padr√µes; as seguintes respondem instantaneamente.
- **Recupera√ß√£o paralela**: quando um campo cr√≠tico falha, as tentativas de recupera√ß√£o s√£o disparadas em paralelo (heur√≠sticas relaxadas ‚Üí prompt dedicado ‚Üí contexto expandido). As decis√µes s√£o logadas como `Field <nome> | recovery_success`.
- **Observabilidade**: `metadata.profiling` acompanha cada resposta, enquanto o backend escreve logs estruturados para cache hits, heur√≠sticas, LLM e recovery (`docker-compose logs -f backend`).
- **UX responsiva**: o frontend mostra o JSON achatado (`flat`), mant√™m os detalhes para exporta√ß√µes e processa lotes com at√© tr√™s uploads simult√¢neos, exibindo progresso parcial.

---

## üìÅ Estrutura do Projeto

```
enter-ai/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extractors/       # L√≥gica de extra√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/          # Orquestra√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Modelos Pydantic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py          # Configura√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ tests/                 # Testes automatizados
‚îÇ   ‚îú‚îÄ‚îÄ scripts/               # Scripts utilit√°rios
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ .env                   # Configura√ß√µes (n√£o versionado)
‚îÇ   ‚îî‚îÄ‚îÄ main.py                # Entry point da API
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ index.html             # Interface web
‚îÇ   ‚îî‚îÄ‚îÄ public/                # Assets est√°ticos
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataset.json       # Dataset de exemplos
‚îÇ   ‚îî‚îÄ‚îÄ files/                 # PDFs de exemplo
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ nginx.conf
‚îî‚îÄ‚îÄ README.md
```

---

## üîß Configura√ß√£o Avan√ßada

### Vari√°veis de Ambiente (backend/.env)

```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-your-key-here
```

### Modelos Suportados
- Todos da OPEN AI

---

## üìù Notas de Desenvolvimento

### Modo de Desenvolvimento com Docker

O docker-compose.yml inclui volumes para hot-reload:
```yaml
volumes:
  - ./backend/app:/app/app  # Altera√ß√µes no c√≥digo refletem automaticamente
```

Para produ√ß√£o, comente esta linha.

### Troubleshooting

**Problema**: API n√£o carrega configura√ß√µes
**Solu√ß√£o**: Verifique se o arquivo `.env` est√° no diret√≥rio `backend/`

**Problema**: Modelo n√£o dispon√≠vel
**Solu√ß√£o**: Verifique sua conta OpenAI e atualize `OPENAI_MODEL` no `.env`

**Problema**: CORS errors no frontend
**Solu√ß√£o**: Certifique-se de que o Nginx est√° configurado corretamente (nginx.conf)

---

## üìÑ Licen√ßa

Este projeto foi desenvolvido como parte do Fellowship ENTER AI.

---

## üë§ Autor

**Matheus** - Fellowship ENTER AI 2025

---
